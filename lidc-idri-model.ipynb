{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b1caa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T19:54:57.188789Z",
     "iopub.status.busy": "2025-06-15T19:54:57.188478Z",
     "iopub.status.idle": "2025-06-15T20:03:45.195592Z",
     "shell.execute_reply": "2025-06-15T20:03:45.194804Z"
    },
    "papermill": {
     "duration": 528.011313,
     "end_time": "2025-06-15T20:03:45.196817",
     "exception": false,
     "start_time": "2025-06-15T19:54:57.185504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 19:55:00.101451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750017300.284807      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750017300.342929      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1750017509.406306      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750017514.486729      59 service.cc:148] XLA service 0x7dd6ac002600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1750017514.487358      59 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1750017514.901459      59 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  5/389\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5201 - loss: 3.0334"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750017518.315951      59 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 50ms/step - accuracy: 0.5813 - loss: 1.1514 - val_accuracy: 0.3830 - val_loss: 3.2601\n",
      "Epoch 2/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6111 - loss: 0.6829 - val_accuracy: 0.6132 - val_loss: 0.6628\n",
      "Epoch 3/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6182 - loss: 0.6678 - val_accuracy: 0.6177 - val_loss: 0.6594\n",
      "Epoch 4/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6099 - loss: 0.6676 - val_accuracy: 0.6177 - val_loss: 0.6588\n",
      "Epoch 5/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6117 - loss: 0.6687 - val_accuracy: 0.6177 - val_loss: 0.6607\n",
      "Epoch 6/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6176 - loss: 0.6660 - val_accuracy: 0.6170 - val_loss: 0.6808\n",
      "Epoch 7/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6154 - loss: 0.6834 - val_accuracy: 0.6177 - val_loss: 0.6648\n",
      "Epoch 8/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6219 - loss: 0.6619 - val_accuracy: 0.6177 - val_loss: 0.6628\n",
      "Epoch 9/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6141 - loss: 0.6656 - val_accuracy: 0.6177 - val_loss: 0.6576\n",
      "Epoch 10/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6183 - loss: 0.6632 - val_accuracy: 0.6177 - val_loss: 0.6594\n",
      "Epoch 11/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6171 - loss: 0.6634 - val_accuracy: 0.6177 - val_loss: 0.6540\n",
      "Epoch 12/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6184 - loss: 0.6663 - val_accuracy: 0.6177 - val_loss: 0.6651\n",
      "Epoch 13/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6128 - loss: 0.6673 - val_accuracy: 0.6177 - val_loss: 0.6624\n",
      "Epoch 14/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6196 - loss: 0.6599 - val_accuracy: 0.6177 - val_loss: 0.6593\n",
      "Epoch 15/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6184 - loss: 0.6611 - val_accuracy: 0.6177 - val_loss: 0.6659\n",
      "Epoch 16/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6146 - loss: 0.6631 - val_accuracy: 0.6177 - val_loss: 0.6655\n",
      "Epoch 17/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6159 - loss: 0.6644 - val_accuracy: 0.6177 - val_loss: 0.6550\n",
      "Epoch 18/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6189 - loss: 0.6587 - val_accuracy: 0.6177 - val_loss: 0.6572\n",
      "Epoch 19/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6144 - loss: 0.6609 - val_accuracy: 0.6177 - val_loss: 0.6522\n",
      "Epoch 20/20\n",
      "\u001b[1m389/389\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6186 - loss: 0.6569 - val_accuracy: 0.6177 - val_loss: 0.6467\n",
      "✅ Saved model to /kaggle/working/lidc_demo_model.h5\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 1: Imports & Settings\n",
    "# --------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "DATA_DIR = '/kaggle/input/lidcidri/LIDC-IDRI-slices'\n",
    "IMG_SIZE = 128\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "MODEL_PATH = '/kaggle/working/lidc_demo_model.h5'\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: Load Slices + Labels\n",
    "# --------------------------\n",
    "def load_data(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for patient in os.listdir(data_dir):\n",
    "        patient_path = os.path.join(data_dir, patient)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "\n",
    "        for nodule in os.listdir(patient_path):\n",
    "            nodule_path = os.path.join(patient_path, nodule)\n",
    "            image_folder = os.path.join(nodule_path, 'images')\n",
    "            if not os.path.isdir(image_folder):\n",
    "                continue\n",
    "\n",
    "            # 🔁 Fake Label: 0 for nodule-0, 1 for nodule-1, etc.\n",
    "            try:\n",
    "                nodule_id = int(nodule.split('-')[-1])\n",
    "                label = nodule_id % 2  # Simulated binary label\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(image_folder):\n",
    "                if file.endswith(('.png', '.jpg')):\n",
    "                    img_path = os.path.join(image_folder, file)\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                    images.append(img)\n",
    "                    labels.append(label)\n",
    "\n",
    "    if not images:\n",
    "        raise ValueError(\"❌ No images found in dataset.\")\n",
    "\n",
    "    X = np.array(images).astype('float32') / 255.0\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "    y = to_categorical(labels, NUM_CLASSES)\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data(DATA_DIR)\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: Augmentation\n",
    "# --------------------------\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: CNN Model\n",
    "# --------------------------\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# --------------------------\n",
    "# Step 5: Training\n",
    "# --------------------------\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1)\n",
    "\n",
    "# --------------------------\n",
    "# Step 6: Save\n",
    "# --------------------------\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"✅ Saved model to {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea52d3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T20:03:45.528677Z",
     "iopub.status.busy": "2025-06-15T20:03:45.527982Z"
    },
    "papermill": {
     "duration": 713.901259,
     "end_time": "2025-06-15T20:15:39.265784",
     "exception": false,
     "start_time": "2025-06-15T20:03:45.364525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 1: Imports & Settings\n",
    "# --------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = '/kaggle/input/lidcidri/LIDC-IDRI-slices'  # Path to your images\n",
    "IMG_SIZE = 224  # EfficientNetB0 input size\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "MODEL_PATH = '/kaggle/working/lidc_transfer_model.h5'\n",
    "\n",
    "# -----------------------------------\n",
    "# Step 2: Load Images & Simulate Labels\n",
    "# -----------------------------------\n",
    "def load_images(data_dir):\n",
    "    X, y = [], []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                X.append(img)\n",
    "                y.append(np.random.randint(0, 2))  # ⚠️ Replace with true labels if available\n",
    "    if not X:\n",
    "        raise ValueError(\"❌ No images found.\")\n",
    "    X = np.array(X).astype('float32') / 255.0\n",
    "    y = to_categorical(y, NUM_CLASSES)\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_images(DATA_DIR)\n",
    "\n",
    "# -----------------------\n",
    "# Step 3: Image Augmentation\n",
    "# -----------------------\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: Build EfficientNetB0 Model\n",
    "# --------------------------\n",
    "def build_efficientnet_model():\n",
    "    base_model = EfficientNetB0(include_top=False, weights='imagenet',\n",
    "                                input_tensor=Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "    base_model.trainable = False  # Freeze base model\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_efficientnet_model()\n",
    "\n",
    "# ------------------------\n",
    "# Step 5: Train the Model\n",
    "# ------------------------\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1)\n",
    "\n",
    "# -----------------------\n",
    "# Step 6: Save Trained Model\n",
    "# -----------------------\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"✅ Saved model to {MODEL_PATH}\")\n",
    "\n",
    "# -----------------------\n",
    "# Step 7: Load + Predict Function\n",
    "# -----------------------\n",
    "def predict_image(model_path, image_path):\n",
    "    model = load_model(model_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found.\")\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)).astype('float32') / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img)[0]\n",
    "    class_id = np.argmax(prediction)\n",
    "    confidence = prediction[class_id]\n",
    "    return class_id, confidence\n",
    "\n",
    "# # Optional usage after training\n",
    "# test_img = '/kaggle/input/lidcidri/LIDC-IDRI-slices/LIDC-IDRI-0001/nodule-0/images/slice-0.png'\n",
    "# pred_class, prob = predict_image(MODEL_PATH, test_img)\n",
    "# print(f\"🧠 Predicted class: {pred_class} with confidence {prob:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1481536,
     "sourceId": 2448052,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1246.773284,
   "end_time": "2025-06-15T20:15:39.906275",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-15T19:54:53.132991",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
